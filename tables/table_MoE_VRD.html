<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Table 2</title>
    <style>
        table {
            width: 70%;
            border-collapse: collapse;
            margin: 20px auto;
        }
        th, td {
            border: 1px solid #ddd;
            padding: 8px;
            text-align: center;
        }
        th {
            background-color: #f2f2f2;
        }
        td.approach {
            text-align: left;
        }
        caption {
            margin: 10px 0;
            text-align: left;
        }
    </style>
</head>
<body>

<table>
    <caption>Table 2: Performance of our proposed MoE-VRD with K = 2 and a total of N = 10 experts, in comparison with state-of-the-art approaches on the ImageNet-VidVRD dataset.</caption>
    <thead>
        <tr>
            <th rowspan="2">Approach</th>
            <th colspan="3">Relation Detection</th>
            <th colspan="4">Relation Tagging</th>
        </tr>
        <tr>
            <th>mAP</th>
            <th>R@50</th>
            <th>R@100</th>
            <th>P@1</th>
            <th>P@5</th>
            <th>P@10</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td class="approach">VidVRD <a href="hhttps://dl.acm.org/doi/abs/10.1145/3123266.3123380" target="_blank">Shang et. al (2017)</a></td>
            <td>8.58</td>
            <td>5.54</td>
            <td>6.37</td>
            <td>43.00</td>
            <td>28.90</td>
            <td>20.80</td>
        </tr>
        <tr>
            <td class="approach">GSTEG <a href="https://openaccess.thecvf.com/content_CVPR_2019/html/Tsai_Video_Relationship_Reasoning_Using_Gated_Spatio-Temporal_Energy_Graph_CVPR_2019_paper.html" target="_blank">Tsai et. al (2019)</a></td>
            <td>9.52</td>
            <td>7.05</td>
            <td>7.67</td>
            <td>51.50</td>
            <td>39.50</td>
            <td>28.23</td>
        </tr>
        <tr>
            <td class="approach">VRD-GCN <a href="https://dl.acm.org/doi/abs/10.1145/3343031.3351058" target="_blank">Qian et. al (2019)</a> </td>
            <td>14.23</td>
            <td>7.43</td>
            <td>8.75</td>
            <td>59.50</td>
            <td>40.50</td>
            <td>27.85</td>
        </tr>
        <tr>
            <td class="approach">3DRN <a href="https://www.sciencedirect.com/science/article/abs/pii/S0925231220319299" target="_blank">Cao et. al (2021)</a></td>
            <td>14.68</td>
            <td>5.53</td>
            <td>6.39</td>
            <td>57.89</td>
            <td>41.80</td>
            <td>29.15</td>
        </tr>
        <tr>
            <td class="approach">VRD-STGC <a href="https://openaccess.thecvf.com/content_CVPR_2019/html/Liu_Structured_Knowledge_Distillation_for_Semantic_Segmentation_CVPR_2019_paper.html" target="_blank">Liu et. al (2019)</a> </td>
            <td>18.38</td>
            <td>11.21</td>
            <td>13.69</td>
            <td>60.00</td>
            <td>43.10</td>
            <td>32.24</td>
        </tr>
        <tr>
            <td class="approach">SFE <a href="https://openaccess.thecvf.com/content/ICCV2021/html/Chen_Social_Fabric_Tubelet_Compositions_for_Video_Relation_Detection_ICCV_2021_paper.html" target="_blank">Chen et. al (2021)</a> </td>
            <td>20.08</td>
            <td>13.73</td>
            <td>16.88</td>
            <td>62.50</td>
            <td>49.20</td>
            <td>38.45</td>
        </tr>
        <tr>
            <td class="approach">IVRD <a href="https://dl.acm.org/doi/abs/10.1145/3474085.3475540" target="_blank">Li et. al (2021)</a> </td>
            <td>22.97</td>
            <td>12.40</td>
            <td>14.46</td>
            <td>68.83</td>
            <td>49.87</td>
            <td>35.57</td>
        </tr>
        <tr>
            <td class="approach">BIG-C <a href="https://openaccess.thecvf.com/content/CVPR2022/html/Gao_Classification-Then-Grounding_Reformulating_Video_Scene_Graphs_As_Temporal_Bipartite_Graphs_CVPR_2022_paper.html" target="_blank">Gao et. al (2022)</a></td>
            <td>26.08</td>
            <td>14.10</td>
            <td>16.25</td>
            <td>73.00</td>
            <td>55.10</td>
            <td>40.00</td>
        </tr>
        <tr>
            <td class="approach">CKERN <a href="https://ieeexplore.ieee.org/abstract/document/9943990" target="_blank">Cao et. al (2022)</a></td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>74.50</td>
            <td>55.59</td>
            <td>41.34</td>
        </tr>
        <tr>
            <td class="approach">VidVRD-II <a href="https://dl.acm.org/doi/abs/10.1145/3474085.3475263" target="_blank">Shang et. al (2021)</a></td>
            <td>29.37 ± 0.40</td>
            <td>19.63 ± 0.19</td>
            <td>22.92 ± 0.48</td>
            <td>70.40 ± 1.53</td>
            <td>53.88 ± 0.31</td>
            <td>40.16 ± 0.70</td>
        </tr>
        <tr>
            <td class="approach">MoE-VRD (K = 2) <b>(ours)</b></td>
            <td><b>33.02 ± 0.23</b></td>
            <td><b>22.77 ± 0.28</b></td>
            <td><b>24.20 ± 0.22</b></td>
            <td><b>74.12 ± 1.44</b></td>
            <td><b>56.47 ± 0.17</b></td>
            <td><b>42.05 ± 0.92</b></td>
        </tr>
    </tbody>
</table>


<table>
    <caption>Table 3: A comparison with the state-of-the-art, as in Table 2, but here on the VidOR dataset.</caption>
    <thead>
        <tr>
            <th rowspan="2">Approach</th>
            <th colspan="3">Relation Detection</th>
            <th colspan="3">Relation Tagging</th>
        </tr>
        <tr>
            <th>mAP</th>
            <th>R@50</th>
            <th>R@100</th>
            <th>P@1</th>
            <th>P@5</th>
            <th>P@10</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td class="approach">3DRN <a href="https://www.sciencedirect.com/science/article/abs/pii/S0925231220319299" target="_blank">Cao et. al (2021)</a></td>
            <td>2.47</td>
            <td>1.58</td>
            <td>1.85</td>
            <td>33.05</td>
            <td>35.27</td>
            <td>-</td>
        </tr>
        <tr>
            <td class="approach">VRD-STGC <a href="https://openaccess.thecvf.com/content_CVPR_2019/html/Liu_Structured_Knowledge_Distillation_for_Semantic_Segmentation_CVPR_2019_paper.html" target="_blank">Liu et. al (2019)</a></td>
            <td>6.85</td>
            <td>8.21</td>
            <td>9.90</td>
            <td>48.92</td>
            <td>36.78</td>
            <td>-</td>
        </tr>
        <tr>
            <td class="approach">IVRD <a href="https://dl.acm.org/doi/abs/10.1145/3474085.3475540" target="_blank">Li et. al (2021)</a></td>
            <td>7.42</td>
            <td>7.36</td>
            <td>9.41</td>
            <td>53.40</td>
            <td>42.70</td>
            <td>-</td>
        </tr>
        <tr>
            <td class="approach">CKERN <a href="https://ieeexplore.ieee.org/abstract/document/9943990" target="_blank">Cao et. al (2022)</a></td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>58.80</td>
            <td>46.07</td>
            <td>34.29</td>
        </tr>
        <tr>
            <td class="approach">BIG <a href="https://openaccess.thecvf.com/content/CVPR2022/html/Gao_Classification-Then-Grounding_Reformulating_Video_Scene_Graphs_As_Temporal_Bipartite_Graphs_CVPR_2022_paper.html" target="_blank">Gao et. al (2022)</a></td>
            <td>8.54</td>
            <td>8.03</td>
            <td>10.04</td>
            <td>64.42</td>
            <td>51.80</td>
            <td>40.96</td>
        </tr>
        <tr>
            <td class="approach">Ens-5 <a href="https://dl.acm.org/doi/abs/10.1145/3474085.3479231" target="_blank">Gao et. al (2021)</a></td>
            <td>9.48</td>
            <td>8.56</td>
            <td>10.43</td>
            <td>63.46</td>
            <td>54.07</td>
            <td><b>41.94</b></td>
        </tr>
        <tr>
            <td class="approach">SFE <a href="https://openaccess.thecvf.com/content/ICCV2021/html/Chen_Social_Fabric_Tubelet_Compositions_for_Video_Relation_Detection_ICCV_2021_paper.html" target="_blank">Chen et. al (2021)</a></td>
            <td><b>11.21</b></td>
            <td><b>9.99</b></td>
            <td><b>11.94</b></td>
            <td><b>68.86</b></td>
            <td><b>55.16</b></td>
            <td>-</td>
        </tr>
        <tr>
            <td class="approach">VidVRD-II <a href="https://dl.acm.org/doi/abs/10.1145/3474085.3475263" target="_blank">Shang et. al (2021)</a></td>
            <td>8.65 ± 0.11</td>
            <td>8.59 ± 0.11</td>
            <td>10.69 ± 0.08</td>
            <td>57.40 ± 0.57</td>
            <td>44.54 ± 0.68</td>
            <td>33.30 ± 0.31</td>
        </tr>
        <tr>
            <td class="approach">MoE-VRD (K = 2) <b>(ours)</b></td>
            <td>9.44 ± 0.21</td>
            <td>9.54 ± 0.13</td>
            <td>11.51 ± 0.31</td>
            <td>58.92 ± 0.67</td>
            <td>45.11 ± 0.19</td>
            <td>34.85 ± 0.22</td>
        </tr>
    </tbody>
</table>

</body>
</html>
